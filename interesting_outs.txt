       Unnamed: 0      Data Godzina        ZAP
0               2  20160101       3  13841.413
1               3  20160101       4  13375.913
2               4  20160101       5  13163.563
3               5  20160101       6  13132.988
4               6  20160101       7  13185.875
5               7  20160101       8  13069.825
6               8  20160101       9  13045.488
7               9  20160101      10  13587.788




#baddata1
       Unnamed: 0      Data Godzina        ZAP
7243         7273  20161030      2A  14049.063
15977       16009  20171029      2A  14375.725



      Unnamed: 0      Data Godzina        ZAP
7241        7271  20161030       1  15208.013
7242        7272  20161030       2  14485.350
7243        7273  20161030      2A  14049.063
7244        7274  20161030       3  13838.200
7245        7275  20161030       4  13789.188


15975       16007  20171029       1  15460.288
15976       16008  20171029       2  14784.313
15977       16009  20171029      2A  14375.725
15978       16010  20171029       3  14146.600
15979       16011  20171029       4  14041.088
15980       16012  20171029       5  14104.675


[19669 rows x 4 columns]



[19702 rows x 1 columns]
brakuje  33 rek



Brakujace dane dziedziny czasu
                     ZAP
2016-03-03 13:00:00  NaN
2016-03-03 14:00:00  NaN
2016-03-03 15:00:00  NaN
2016-03-03 16:00:00  NaN
2016-03-03 17:00:00  NaN
2016-03-03 18:00:00  NaN
2016-03-03 19:00:00  NaN
2016-03-03 20:00:00  NaN
2016-03-03 21:00:00  NaN
2016-03-03 22:00:00  NaN
2016-03-03 23:00:00  NaN
2016-03-04 00:00:00  NaN
2016-03-04 01:00:00  NaN
2016-03-04 02:00:00  NaN
2016-03-04 03:00:00  NaN
2016-03-04 04:00:00  NaN
2016-03-04 05:00:00  NaN
2016-03-04 06:00:00  NaN
2016-03-04 07:00:00  NaN
2016-03-04 08:00:00  NaN
2016-03-04 09:00:00  NaN
2016-03-04 10:00:00  NaN
2016-03-04 11:00:00  NaN
2016-03-04 12:00:00  NaN
2016-03-04 13:00:00  NaN
2016-03-04 14:00:00  NaN
2016-03-27 02:00:00  NaN
2016-05-18 23:00:00  NaN
2016-08-19 13:00:00  NaN
2017-02-20 17:00:00  NaN
2017-03-26 02:00:00  NaN
2017-04-08 01:00:00  NaN
2018-03-25 02:00:00  NaN



#uczenie
Layer (type)                 Output Shape              Param #   
=================================================================
gru (GRU)                    (None, None, 32)          3264      
_________________________________________________________________
dense (Dense)                (None, None, 1)           33        
=================================================================
Total params: 3,297
Trainable params: 3,297
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20
2019-03-30 11:20:26.838786: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3350660000 Hz
2019-03-30 11:20:26.852631: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x6470420 executing computations on platform Host. Devices:
2019-03-30 11:20:26.852705: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2019-03-30 11:20:28.465244: W tensorflow/core/framework/allocator.cc:124] Allocation of 44040192 exceeds 10% of system memory.
2019-03-30 11:20:28.526590: W tensorflow/core/framework/allocator.cc:124] Allocation of 44040192 exceeds 10% of system memory.
2019-03-30 11:20:28.680400: W tensorflow/core/framework/allocator.cc:124] Allocation of 44040192 exceeds 10% of system memory.
2019-03-30 11:20:28.710382: W tensorflow/core/framework/allocator.cc:124] Allocation of 44040192 exceeds 10% of system memory.
  1/100 [..............................] - ETA: 4:28 - loss: 0.04112019-03-30 11:20:29.643838: W tensorflow/core/framework/allocator.cc:124] Allocation of 44040192 exceeds 10% of system memory.
1/1 [==============================] - 0s 256ms/sample - loss: 0.0253

Epoch 00001: val_loss improved from inf to 0.02528, saving model to 23_checkpoint.keras
100/100 [==============================] - 90s 899ms/step - loss: 0.0264 - val_loss: 0.0253
Epoch 2/20
1/1 [==============================] - 0s 138ms/sample - loss: 0.0225

Epoch 00002: val_loss improved from 0.02528 to 0.02250, saving model to 23_checkpoint.keras
100/100 [==============================] - 96s 959ms/step - loss: 0.0161 - val_loss: 0.0225
Epoch 3/20
1/1 [==============================] - 0s 148ms/sample - loss: 0.0239

Epoch 00003: val_loss did not improve from 0.02250

Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00010000000475.
100/100 [==============================] - 97s 968ms/step - loss: 0.0143 - val_loss: 0.0239
Epoch 4/20
1/1 [==============================] - 0s 139ms/sample - loss: 0.0197

Epoch 00004: val_loss improved from 0.02250 to 0.01973, saving model to 23_checkpoint.keras
100/100 [==============================] - 92s 922ms/step - loss: 0.0132 - val_loss: 0.0197
Epoch 5/20
1/1 [==============================] - 0s 137ms/sample - loss: 0.0202

Epoch 00005: val_loss did not improve from 0.01973

Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0001.
100/100 [==============================] - 89s 894ms/step - loss: 0.0126 - val_loss: 0.0202
Epoch 6/20
1/1 [==============================] - 0s 134ms/sample - loss: 0.0204

Epoch 00006: val_loss did not improve from 0.01973
100/100 [==============================] - 90s 898ms/step - loss: 0.0122 - val_loss: 0.0204
Epoch 7/20
1/1 [==============================] - 0s 134ms/sample - loss: 0.0207

Epoch 00007: val_loss did not improve from 0.01973
100/100 [==============================] - 89s 890ms/step - loss: 0.0118 - val_loss: 0.0207
Epoch 8/20
1/1 [==============================] - 0s 135ms/sample - loss: 0.0199

Epoch 00008: val_loss did not improve from 0.01973
100/100 [==============================] - 89s 893ms/step - loss: 0.0115 - val_loss: 0.0199
Epoch 9/20
1/1 [==============================] - 0s 163ms/sample - loss: 0.0200

Epoch 00009: val_loss did not improve from 0.01973
100/100 [==============================] - 96s 962ms/step - loss: 0.0112 - val_loss: 0.0200
Epoch 00009: early stopping

